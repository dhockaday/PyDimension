{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from google_drive_downloader import GoogleDriveDownloader as gdd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from numpy import linalg as LA\n",
    "import pandas as pd\n",
    "import pysindy as ps\n",
    "from tqdm import tqdm\n",
    "from termcolor import colored\n",
    "from scipy.optimize import minimize\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import yaml\n",
    "\n",
    "from utils.PolyDiff import PolyDiffPoint\n",
    "from utils.SeqReg import SeqReg\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams[\"font.family\"] = 'Arial'\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FitEqu(object):\n",
    "    '''\n",
    "    Sparse regression to fit a equation\n",
    "\n",
    "    1. create dataset\n",
    "    2. preprocess dataset\n",
    "    3. build library\n",
    "    4. fit equation\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        super(FitEqu, self).__init__()\n",
    "\n",
    "    @staticmethod\n",
    "    def load_dataset(case_id):\n",
    "        '''\n",
    "        load dataset\n",
    "        '''\n",
    "        U = np.load(f'../dataset/vorticity-equ/{case_id}/u_3D.npy')\n",
    "        V = np.load(f'../dataset/vorticity-equ/{case_id}/v_3D.npy')\n",
    "        W = np.load(f'../dataset/vorticity-equ/{case_id}/w_3D.npy')\n",
    "\n",
    "        W = np.transpose(W, axes=[2, 1, 0])  # x, y, t\n",
    "        U = np.transpose(U, axes=[2, 1, 0])\n",
    "        V = np.transpose(V, axes=[2, 1, 0])\n",
    "\n",
    "        return U, V, W\n",
    "    \n",
    "    @staticmethod\n",
    "    def add_noise(U, V, W, noise_level=0.01):\n",
    "        Nx, Ny, Nt = U.shape\n",
    "        np.random.seed(0)\n",
    "        U = U + noise_level * np.std(U) * np.random.randn(Nx, Ny, Nt)\n",
    "        V = V + noise_level * np.std(V) * np.random.randn(Nx, Ny, Nt)\n",
    "        W = W + noise_level * np.std(W) * np.random.randn(Nx, Ny, Nt)\n",
    "        return U, V, W\n",
    "    \n",
    "    @staticmethod\n",
    "    def denoise(U, V, W, SVD_mode, is_show=False):\n",
    "        '''\n",
    "        SVD denoise\n",
    "        '''\n",
    "        Nx, Ny, Nt = U.shape\n",
    "        U = U.reshape(Nx * Ny, Nt)\n",
    "        V = V.reshape(Nx * Ny, Nt)\n",
    "        W = W.reshape(Nx * Ny, Nt)\n",
    "\n",
    "        uw, sigmaw, vw = np.linalg.svd(W, full_matrices=False); vw = vw.T\n",
    "        uu, sigmau, vu = np.linalg.svd(U, full_matrices=False); vu = vu.T\n",
    "        uv, sigmav, vv = np.linalg.svd(V, full_matrices=False); vv = vv.T\n",
    "\n",
    "        if is_show:\n",
    "            # singular values of clean data\n",
    "            fig = plt.figure()\n",
    "            plt.semilogy(np.arange(Nt), sigmaw, 'r', label = r'$\\omega$')\n",
    "            plt.semilogy(np.arange(Nt), sigmau, 'g', label = r'$u$')\n",
    "            plt.semilogy(np.arange(Nt), sigmav, 'b', label = r'$v$')\n",
    "\n",
    "            plt.legend(loc = 'upper right', fontsize = 20)\n",
    "            plt.xticks(fontsize = 16)\n",
    "            plt.yticks(fontsize = 16)\n",
    "            plt.xlim([0, 10])\n",
    "\n",
    "        mode_u, mode_v, mode_w = SVD_mode\n",
    "        W = uw[:,0:mode_w].dot(np.diag(sigmaw[0:mode_w]).dot(vw[:,0:mode_w].T)).reshape(Nx, Ny, Nt)\n",
    "        U = uu[:,0:mode_u].dot(np.diag(sigmau[0:mode_u]).dot(vu[:,0:mode_u].T)).reshape(Nx, Ny, Nt)\n",
    "        V = uv[:,0:mode_v].dot(np.diag(sigmav[0:mode_v]).dot(vv[:,0:mode_v].T)).reshape(Nx, Ny, Nt)\n",
    "\n",
    "        return U, V, W\n",
    "        \n",
    "    @staticmethod\n",
    "    def select_sub_domain(U, V, W, x_range, y_range):\n",
    "        '''\n",
    "        select a sub-domain\n",
    "        '''\n",
    "        # select a sub-domain\n",
    "        U = U[x_range[0]:x_range[1], y_range[0]:y_range[1], :]\n",
    "        V = V[x_range[0]:x_range[1], y_range[0]:y_range[1], :]\n",
    "        W = W[x_range[0]:x_range[1], y_range[0]:y_range[1], :]            \n",
    "        return U, V, W\n",
    "    \n",
    "    @staticmethod\n",
    "    def non_dimensionlize(U, V, W, dx, dy, dt, ref_params, sim_params):\n",
    "        '''\n",
    "        non-dimensionlize dx, dy, and dt\n",
    "        '''\n",
    "        # reference parameters\n",
    "        l_ref, v_ref = ref_params['l_ref'], ref_params['v_ref']\n",
    "        # l_ref, v_ref = ref_params['l_ref'], ref_params['v_ref']\n",
    "        t_ref, w_ref = l_ref / v_ref, v_ref / l_ref\n",
    "\n",
    "        # non-dimensionlize data\n",
    "        U, V, W = U/v_ref, V/v_ref, W/w_ref\n",
    "        dx, dy, dt = dx / l_ref, dy / l_ref, dt / t_ref\n",
    "\n",
    "        # analyze the dimensionless numbers\n",
    "        Re = v_ref * l_ref * sim_params['rho'] / sim_params['mu']\n",
    "        coef_best = 1 / Re\n",
    "        print(colored(f'Re: {int(Re)}, coef_best: {round(coef_best, 6)}', 'red'))\n",
    "        return U, V, W, dx, dy, dt, Re, coef_best\n",
    "\n",
    "    @staticmethod\n",
    "    def sample_points(Nx, Ny, boundary_x, boundary_y, boundary_t):\n",
    "        '''\n",
    "        sample a certain number of points to train\n",
    "        '''\n",
    "        np.random.seed(0)\n",
    "        points, count = {}, 0\n",
    "\n",
    "        num_xy = 500\n",
    "        for _ in range(num_xy):\n",
    "            x = np.random.choice(np.arange(boundary_x, Nx-boundary_x), 1)[0]\n",
    "            y = np.random.choice(np.arange(boundary_y, Ny-boundary_y), 1)[0]\n",
    "            for t in range(20, 100, 10):  # for Re=50, 70, 100, 150\n",
    "            # for t in range(300, 490, 10):  # for Re=200\n",
    "                points[count] = [x, y, t]\n",
    "                count = count + 1\n",
    "        return points\n",
    "\n",
    "    @staticmethod\n",
    "    def cal_derivatives(U, V, W, points, boundary_x, boundary_y, boundary_t, dx, dy, dt, deg=5):\n",
    "        num_points = len(points)\n",
    "        w = np.zeros((num_points,1))\n",
    "        u = np.zeros((num_points,1))\n",
    "        v = np.zeros((num_points,1))\n",
    "        wt = np.zeros((num_points,1))\n",
    "        wx = np.zeros((num_points,1))\n",
    "        wy = np.zeros((num_points,1))\n",
    "        wxx = np.zeros((num_points,1))\n",
    "        wxy = np.zeros((num_points,1))\n",
    "        wyy = np.zeros((num_points,1))\n",
    "\n",
    "        Nx_sample, Ny_sample, Nt_sample = 2 * boundary_x - 1, 2 * boundary_y - 1, 2 * boundary_t - 1\n",
    "\n",
    "        for p in tqdm(points.keys()):\n",
    "            [x, y, t] = points[p]\n",
    "            w[p] = W[x, y, t]\n",
    "            u[p] = U[x, y, t]\n",
    "            v[p] = V[x, y, t]\n",
    "            \n",
    "            wt_part = W[x, y, t-int((Nt_sample-1)/2):t+int((Nt_sample+1)/2)]\n",
    "            wx_part = W[x-int((Nx_sample-1)/2):x+int((Nx_sample+1)/2), y, t]\n",
    "            wy_part = W[x,y-int((Ny_sample-1)/2):y+int((Ny_sample+1)/2),t]\n",
    "            wx_part_yp = W[x-int((Nx_sample-1)/2):x+int((Nx_sample+1)/2), y+1, t]\n",
    "            wx_part_ym = W[x-int((Nx_sample-1)/2):x+int((Nx_sample+1)/2), y-1, t]\n",
    "\n",
    "            wt[p] = PolyDiffPoint(wt_part, np.arange(Nt_sample)*dt, deg, 1)[0]\n",
    "            x_diff = PolyDiffPoint(wx_part, np.arange(Nx_sample)*dx, deg, 2)\n",
    "            y_diff = PolyDiffPoint(wy_part, np.arange(Ny_sample)*dy, deg, 2)\n",
    "            x_diff_yp = PolyDiffPoint(wx_part_yp, np.arange(Nx_sample)*dx, deg, 2)\n",
    "            x_diff_ym = PolyDiffPoint(wx_part_ym, np.arange(Nx_sample)*dx, deg, 2)\n",
    "\n",
    "            wx[p], wxx[p] = x_diff[0], x_diff[1]\n",
    "            wy[p], wyy[p] = y_diff[0], y_diff[1]\n",
    "            wxy[p] = (x_diff_yp[0] - x_diff_ym[0]) / (2 * dy)\n",
    "        \n",
    "        base_library = [w, u, v, wt, wx, wy, wxx, wxy, wyy]\n",
    "        return base_library\n",
    "\n",
    "    @staticmethod\n",
    "    def parse_library(base_library):\n",
    "        '''\n",
    "        calculate derivatives and build library for sparse regression\n",
    "        '''\n",
    "        w, u, v, wt, wx, wy, wxx, wxy, wyy = base_library\n",
    "        #########################rotationial invariance#########################\n",
    "        u_ = np.concatenate([u, v])\n",
    "        v_ = np.concatenate([v, u])\n",
    "        w_ = np.concatenate([w, -w])\n",
    "        wx_ = np.concatenate([wx, -wy])\n",
    "        wy_ = np.concatenate([wy, -wx])\n",
    "        wxx_ = np.concatenate([wxx, -wyy])\n",
    "        wyy_ = np.concatenate([wyy, -wxx])\n",
    "        wxy_ = np.concatenate([wxy, -wxy])\n",
    "        u, v, w, wx, wy, wxx, wyy, wxy = u_, v_, w_, wx_, wy_, wxx_, wyy_, wxy_\n",
    "\n",
    "        wt = np.concatenate([wt, -wt])\n",
    "\n",
    "        # # 4 terms\n",
    "        # X_library = [u*wx, v*wy, wxx, wyy]\n",
    "        # names = ['u*wx', 'v*wy', 'wxx', 'wyy']\n",
    "        # 29 terms\n",
    "        X_library = [\n",
    "            u, v, w, wx, wy, wxx, wyy, wxy,\n",
    "            u*u, u*v, u*w, u*wx, u*wy, u*wxx, u*wyy, u*wxy,\n",
    "            v*v, v*w, v*wx, v*wy, v*wxx, v*wyy, v*wxy, \n",
    "            w*w, w*wx, w*wy, w*wxx, w*wyy, w*wxy, \n",
    "        ]\n",
    "        names = [\n",
    "            'u', 'v', 'w', 'wx', 'wy', 'wxx', 'wyy', 'wxy',\n",
    "            'u*u', 'u*v', 'u*w', 'u*wx', 'u*wy', 'u*wxx', 'u*wyy', 'u*wxy',\n",
    "            'v*v', 'v*w', 'v*wx', 'v*wy', 'v*wxx', 'v*wyy', 'v*wxy',\n",
    "            'w*w', 'w*wx', 'w*wy', 'w*wxx', 'w*wyy', 'w*wxy',\n",
    "        ]\n",
    "        X_library = np.squeeze(np.stack(X_library, axis=-1))\n",
    "        y_library = wt.reshape(-1, 1)\n",
    "\n",
    "        # rescale the data\n",
    "        norm_coef = np.mean(np.abs(np.mean(X_library, axis=0)))\n",
    "        X_library = X_library / norm_coef\n",
    "        y_library = y_library / norm_coef\n",
    "        return X_library, y_library, names\n",
    "\n",
    "    @staticmethod\n",
    "    def check_library(X_library, y_library):\n",
    "        '''\n",
    "        check whether the library has any Nan\n",
    "        '''\n",
    "        if np.any(np.isnan(X_library)) or np.any(np.isnan(y_library)):\n",
    "            print('Nan exists in library.')\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    @staticmethod\n",
    "    def check_equ(X_library, y_library, Re):\n",
    "        '''\n",
    "        check the r2 for the target equation: wt = -u*wx - v*wy + 1/Re(wxx + wyy)\n",
    "        '''\n",
    "        pred_best = (-X_library[:, 11] \\\n",
    "                    -X_library[:, 19] \\\n",
    "                    + 1/Re*(X_library[:, 5] + X_library[:, 6])).reshape(-1,1)\n",
    "        r2 = r2_score(y_library, pred_best)\n",
    "        print(f\"Analytical r2_score: {round(r2, 6)}\")\n",
    "\n",
    "        pred_best = (-X_library[:, 11] \\\n",
    "                    -X_library[:, 19]).reshape(-1,1)\n",
    "        r2 = r2_score(y_library, pred_best)\n",
    "        print(f\"Analytical r2_score (no 1/Re): {round(r2, 6)}\")\n",
    "        return None\n",
    "        \n",
    "    @staticmethod\n",
    "    def fit(X_library, y_library, threshold=0.002):\n",
    "        '''\n",
    "        squential threshold with dynamic threshold\n",
    "        '''\n",
    "        model = SeqReg()\n",
    "        coef, _, r2 = model.fit_dynamic_thresh(X_library, y_library, \n",
    "                        is_normalize=False, non_zero_term=5, threshold=threshold, fit_intercept=False, model_name='LR')\n",
    "        print('Fitting r2', r2)\n",
    "        return coef\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading 1E295SWc1JjvrPmLcVyzLmAsV5kaBi69Z into ../dataset/vorticity-equ.zip... Done.\n",
      "Unzipping..."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xie/anaconda3/envs/PyDimension/lib/python3.11/site-packages/google_drive_downloader/google_drive_downloader.py:78: UserWarning: Ignoring `unzip` since \"1E295SWc1JjvrPmLcVyzLmAsV5kaBi69Z\" does not look like a valid zip file\n",
      "  warnings.warn('Ignoring `unzip` since \"{}\" does not look like a valid zip file'.format(file_id))\n"
     ]
    }
   ],
   "source": [
    "# load config file\n",
    "config_path = 'configs/config_vorticity_3cylinder_noise.yml'\n",
    "config = yaml.load(open(config_path, 'r'), Loader=yaml.FullLoader)\n",
    "case_id_list = list(config['case_id_list'].keys())\n",
    "\n",
    "# download dataset\n",
    "def download_dataset():\n",
    "    '''Download dataset from google drive'''\n",
    "    gdd.download_file_from_google_drive(\n",
    "        file_id=config['dataset']['data_url'][32:65],\n",
    "        dest_path=config['dataset']['dest_path'],\n",
    "        unzip=True\n",
    "    )\n",
    "\n",
    "if config['dataset']['need_download']:\n",
    "    download_dataset()\n",
    "else:\n",
    "    print('Please make sure you have downloaded the dataset and put them into dataset folder.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************** v2-Re-50\n",
      "\u001b[31mRe: 50, coef_best: 0.02\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4000/4000 [00:05<00:00, 798.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analytical r2_score: 0.988834\n",
      "Analytical r2_score (no 1/Re): -0.020446\n",
      "Fitting r2 0.9896583113534032\n",
      "\u001b[31mcoef_res: [('u*wx', -0.999409), ('v*wy', -0.999409), ('wxx', 0.020576), ('wyy', 0.020576)]\u001b[0m\n",
      "**************************************** v2-Re-90\n",
      "\u001b[31mRe: 90, coef_best: 0.011111\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4000/4000 [00:04<00:00, 800.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analytical r2_score: 0.991511\n",
      "Analytical r2_score (no 1/Re): 0.722509\n",
      "Fitting r2 0.9943157104295343\n",
      "\u001b[31mcoef_res: [('u*wx', -0.998253), ('v*wy', -0.998253), ('wxx', 0.012346), ('wyy', 0.012346)]\u001b[0m\n",
      "**************************************** v2-Re-100\n",
      "\u001b[31mRe: 100, coef_best: 0.01\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4000/4000 [00:04<00:00, 810.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analytical r2_score: 0.993797\n",
      "Analytical r2_score (no 1/Re): 0.821554\n",
      "Fitting r2 0.9957578020714153\n",
      "\u001b[31mcoef_res: [('u*wx', -0.99255), ('v*wy', -0.99255), ('wxx', 0.011085), ('wyy', 0.011085)]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "def prepare_dataset(is_show=False):\n",
    "    '''\n",
    "    prepare a sets of dataset\n",
    "    '''\n",
    "    res = []\n",
    "    fit_equ = FitEqu()\n",
    "\n",
    "    for case_id in case_id_list:\n",
    "        print('*' * 40, case_id)\n",
    "        case_info = config['case_id_list'][case_id]\n",
    "        Nx, Ny = 500, 222\n",
    "        dx, dy= 0.9/Nx, 0.4/Ny\n",
    "\n",
    "        ref_params = case_info['ref_params']\n",
    "        sim_params = case_info['sim_params']\n",
    "        sim_params['niu'] = sim_params['mu'] / sim_params['rho']\n",
    "        dt = sim_params['dt']\n",
    "\n",
    "        # parameters for sparse regression\n",
    "        x_range = case_info['x_range']\n",
    "        y_range = case_info['y_range']\n",
    "        boundary_x, boundary_y, boundary_t = case_info['fitting']['boundary_num']  # default is 5\n",
    "        deg = case_info['fitting']['deg']               # polynomial degree default is 3\n",
    "        \n",
    "        #####################Prepare Dataset#####################\n",
    "        # generate data and save to a folder\n",
    "        U, V, W = fit_equ.load_dataset(case_id)\n",
    "        U, V, W = fit_equ.select_sub_domain(U, V, W, x_range, y_range)\n",
    "        U, V, W = fit_equ.add_noise(U, V, W, noise_level=0.01)\n",
    "        U, V, W = fit_equ.denoise(U, V, W, case_info['SVD_mode'])\n",
    "        U, V, W, dx, dy, dt, Re, coef_best = fit_equ.non_dimensionlize(U, V, W, dx, dy, dt, ref_params, sim_params)\n",
    "        Nx, Ny, Nt = W.shape\n",
    "        points = fit_equ.sample_points(Nx, Ny, boundary_x, boundary_y, boundary_t)        \n",
    "        if is_show: fig = plt.figure(figsize=(6, 4)); plt.imshow(W[:, :, 0].T) # plot the 1st frame \n",
    "\n",
    "        #####################Prepare library#####################\n",
    "        base_library = fit_equ.cal_derivatives(\n",
    "            U, V, W, points, boundary_x, boundary_y, boundary_t, dx, dy, dt, deg)\n",
    "        X_library, y_library, names = fit_equ.parse_library(base_library)\n",
    "        is_nan = fit_equ.check_library(X_library, y_library) # check data\n",
    "        if is_nan: continue\n",
    "        df_all = pd.DataFrame(np.concatenate([X_library, y_library], axis=1))\n",
    "        # check the fitting of the target equation\n",
    "        fit_equ.check_equ(X_library, y_library, Re)\n",
    "        # visualization library\n",
    "        if is_show: fig = plt.figure(); df_all.plot(); plt.xlim([0, 2000])\n",
    "\n",
    "        #####################Fit sequential sparse regression#####################\n",
    "        coef = fit_equ.fit(X_library, y_library)\n",
    "        coef_res = [(each[0], round(each[1], 6)) for each in list(zip(names, coef.tolist())) if abs(each[1]) >= 1e-3]\n",
    "        coef_res = sorted(coef_res, key=lambda x: abs(x[1]), reverse=True)\n",
    "        print(colored(f'coef_res: {coef_res}', 'red'))\n",
    "        # add to the total results\n",
    "        res.append([sim_params['niu'], ref_params['l_ref'], ref_params['v_ref'], \n",
    "                sim_params['rho'], ref_params['p_ref'], Nx*dx, Ny*dy, Re, coef_best] + coef.tolist())\n",
    "    \n",
    "    df = pd.DataFrame(res)\n",
    "    return df\n",
    "\n",
    "df = prepare_dataset()\n",
    "df.to_csv('../dataset/dataset_vorticity_flow3D.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../dataset/dataset_vorticity_flow3D.csv')\n",
    "df = df.rename(columns={\n",
    "    '0': 'niu',\n",
    "    '1': 'diameter',\n",
    "    '2': 'v_init',\n",
    "    '3': 'density',\n",
    "    '4': 'p_init',\n",
    "    '5': 'lx',\n",
    "    '6': 'ly',\n",
    "    '7': 'Re',\n",
    "    '8': 'coef_best',\n",
    "    })\n",
    "df['mu'] = df['niu'] * df['density']\n",
    "# print(df.head())\n",
    "df.to_csv('../dataset/dataset_vorticity_flow3D.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Re  coef_best        14        20\n",
      "0   50.0   0.020000  0.020576 -0.999409\n",
      "1   90.0   0.011111  0.012346 -0.998253\n",
      "2  100.0   0.010000  0.011085 -0.992550\n"
     ]
    }
   ],
   "source": [
    "print(df[['Re', 'coef_best', '14', '20']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final r2 0.9822067779393626 [ 1. -1. -1. -1.  0.] [[1.05788549]]\n",
      "final r2 0.9822067779393626 [ 1. -1. -1. -1.  0.] [[1.05788549]]\n"
     ]
    }
   ],
   "source": [
    "class DimensionlessLearning(object):\n",
    "    '''\n",
    "    Indentify the explicit form one coefficient using dimensionless learning\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, df, input_list, output_coef, dimension_info, basis_list):\n",
    "        super(DimensionlessLearning, self).__init__()\n",
    "        self.df = df\n",
    "        self.input_list = input_list\n",
    "        self.output_coef = output_coef\n",
    "        self.X, self.y = self.prepare_dataset()\n",
    "        self.dimension_info, self.basis_list = dimension_info, basis_list\n",
    "        self.basis1_in, self.basis2_in = self.prepare_dimension()\n",
    "\n",
    "    def prepare_dataset(self):\n",
    "        '''\n",
    "        prepare the input and output data\n",
    "        '''\n",
    "        X = self.df[self.input_list].to_numpy()\n",
    "        y = self.df[self.output_coef].to_numpy().reshape(-1, 1)\n",
    "        return X, y\n",
    "        \n",
    "    def prepare_dimension(self):\n",
    "        '''\n",
    "        parse dimension for input and output\n",
    "        '''\n",
    "        basis1_in, basis2_in = self.basis_list[0], self.basis_list[1]\n",
    "        return basis1_in, basis2_in\n",
    "\n",
    "    def fetch_coef_pi(self, coef):\n",
    "        '''\n",
    "        parse the combined weights for the input\n",
    "        '''\n",
    "        coef_pi = coef[0] * self.basis1_in + coef[1] * self.basis2_in\n",
    "        return coef_pi\n",
    "        \n",
    "    def check_dimension(self, coef):\n",
    "        '''\n",
    "        check whether the basis vectors can formulated as the D_out\n",
    "        '''\n",
    "        coef_pi = self.fetch_coef_pi(coef)\n",
    "        # print('[check] coef_pi: \\n', coef_pi)\n",
    "        target_D_out = np.dot(self.dimension_info[0], coef_pi)\n",
    "        # print('[check] target_D_out: \\n', target_D_out)\n",
    "        assert np.array_equal(target_D_out, self.dimension_info[1]), 'Wrong target_D_out!'\n",
    "\n",
    "    def fit_pattern_search(self, seed):\n",
    "        '''\n",
    "        pattern search\n",
    "        '''\n",
    "        def get_coordinates(a, b, delta):\n",
    "            '''\n",
    "            Build a list to store all possible coordiantes\n",
    "            '''\n",
    "            coord_all = []\n",
    "            for a_ in [a-delta, a, a+delta]:\n",
    "                for b_ in [b-delta, b, b+delta]:\n",
    "                    if [a_, b_] != [a, b]:\n",
    "                        coord_all.append([a_, b_])\n",
    "            return coord_all\n",
    "        \n",
    "        def opt(coef):\n",
    "            '''\n",
    "            fit a linear regression\n",
    "            '''\n",
    "            coef_pi = self.fetch_coef_pi(coef)\n",
    "            pi_in = np.prod(np.power(self.X, coef_pi.reshape(-1,)), axis=1).reshape(-1, 1)\n",
    "            reg =LinearRegression(fit_intercept=False)\n",
    "            reg.fit(pi_in, self.y)\n",
    "            y_pred = reg.predict(pi_in)\n",
    "            r2 = r2_score(self.y, y_pred)\n",
    "            return r2, coef_pi, reg.coef_\n",
    "\n",
    "        np.random.seed(seed)\n",
    "        res, break_points = [], []\n",
    "        a = np.random.choice(np.linspace(-2, 2, 9), 1)[0]  # [-2, 2] delta=0.5\n",
    "        b = np.random.choice(np.linspace(-2, 2, 9), 1)[0]  # [-2, 2] delta=0.5\n",
    "        # a, b = -1, 0\n",
    "        coef = np.array([a, b]).reshape(-1, 1)\n",
    "\n",
    "        iter_num, max_iter, delta = 0, 10, 0.5\n",
    "        while iter_num < max_iter:\n",
    "            candidate_coord = get_coordinates(a, b, delta)\n",
    "            r2_center, reg_coef_center, coef_w_center = opt(coef)\n",
    "            # print('r2_center', round(r2_center, 2), 'reg_coef_center', [round(each, 2) for each in list(reg_coef_center.reshape(-1,))])\n",
    "            # print('coef_w_center', coef_w_center)\n",
    "\n",
    "            if r2_center < 0.2:\n",
    "                break_points.append([a, b])\n",
    "                break\n",
    "\n",
    "            r2_bounds_val = []\n",
    "            for [a_, b_] in candidate_coord:\n",
    "                coef_temp = np.array([a_, b_]).reshape(-1, 1)\n",
    "                r2_bound, reg_coef_bound, coef_w_bound = opt(coef_temp)\n",
    "                r2_bounds_val.append(r2_bound)\n",
    "\n",
    "            # sort r2 from high to low\n",
    "            highest_index = np.argsort(r2_bounds_val)[::-1][0]\n",
    "            iter_num += 1\n",
    "\n",
    "            # udpate the center coordiantes when the R2 in the neighborhood is higher\n",
    "            if r2_center < r2_bounds_val[highest_index]:\n",
    "                [a, b] = candidate_coord[highest_index]\n",
    "                coef = np.array([a, b]).reshape(-1, 1)\n",
    "                coef_pi = self.fetch_coef_pi(coef)\n",
    "                res_info = {'a': a, 'b': b, 'r2_center': round(r2_bounds_val[highest_index], 4)}\n",
    "                # print('update', res_info)\n",
    "                res.append(res_info)\n",
    "            else:\n",
    "                break\n",
    "        \n",
    "        coef_pi = self.fetch_coef_pi(coef)\n",
    "        r2, reg_coef_final, coef_w_final = opt(coef)\n",
    "        return r2, reg_coef_final, coef_w_final\n",
    "\n",
    "\n",
    "def recover_coef1(seed):\n",
    "    input_list = ['mu', 'diameter', 'v_init', 'density', 'p_init']\n",
    "    output_coef = '14'\n",
    "\n",
    "    D_in = np.mat('-1, -1, 1; 1, 0, 0; 1, -1, 0; -3, 0, 1; -1, -2, 1').T\n",
    "    D_out = np.mat('0;, 0; 0')\n",
    "    dimension_info = [D_in, D_out]\n",
    "\n",
    "    basis1_in = np.array([-1, 1, 1, 1, 0]).reshape(-1, 1)\n",
    "    basis2_in = np.array([-1, 1, -1, 0, 1]).reshape(-1, 1)\n",
    "    basis_list = [basis1_in, basis2_in]\n",
    "    \n",
    "    dimensionless_learning = DimensionlessLearning(\n",
    "        df, input_list, output_coef, dimension_info, basis_list)\n",
    "    # dimensionless_learning.check_dimension(coef=[-1, 0])\n",
    "\n",
    "    # pattern search\n",
    "    r2, coef, coef_w = dimensionless_learning.fit_pattern_search(seed=seed)\n",
    "    if r2 > 0.8:\n",
    "        print('final r2', r2, coef.flatten(), coef_w)\n",
    "\n",
    "\n",
    "for i in range(10):\n",
    "    recover_coef1(seed=i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "PyDimension",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0 | packaged by conda-forge | (main, Oct 25 2022, 06:24:40) [GCC 10.4.0]"
  },
  "vscode": {
   "interpreter": {
    "hash": "904ec0901b1dab60cbb175cce6563014087aab52b2a683f993104952735b0829"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
